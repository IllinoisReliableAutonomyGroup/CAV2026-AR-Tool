\section{Introduction}
\label{sec:introduction}

Certification of vision-based systems under semantic uncertainties is a central challenge with applications in safety-critical domains such as automotive (lane keeping~\cite{yin2025certified,9852797} and emergency braking~\cite{yin2025certified}), avionics (navigation\- and\- landing \cite{semerikov2025vision,santa2022nnlander,li2023refiningperceptioncontractscase}), and robotic manipulation~\cite{hu2022robustness}. Semantic uncertainties refer to variations in the scene being perceived by the vision system as well as the camera parameters such as its pose. Traditional robustness verification techniques for vision models primarily focus on pixel-level perturbations (e.g., adversarial attacks~\cite{szegedy2014intriguingpropertiesneuralnetworks,kurakin2017adversarialexamplesphysicalworld,madry2018towards,8294186,9614158}), and do not capture the rich set of semantic variations that affect the rendered images dramatically.
With the emergence of 3D reconstruction techniques such as Neural Radiance Fields (NeRFs)~\cite{10.1145/3503250} and 3D Gaussian Splats (3DGS)~\cite{10.1145/3592433}, it has become possible to learn photorealistic 3D representations of scenes from images and render novel views. This progress has both scaled up data generation for vision and robotics~\cite{falcon} and opened the door to formal verification of vision models in photorealistic scenes with semantic uncertainty.


Abstract rendering~\cite{AbstractRendering_Neurips2025} is a  technique for over-approximating the set of all possible rendered images that can result from a set of camera poses and 3D scenes with semantic uncertainties. In a nutshell, abstract rendering takes as input a bounded set of camera poses---defined by interval pose matrices---and a set of scenes represented by 3D Gaussian spats (3DGS)~\cite{10.1145/3592433}, and propagates these sets through the standard 3DGS rendering algorithm to compute the output set of {\em abstract images}, which are represented by per-pixel color intervals or linear bounds. The key enabler for this technique is a set of piecewise linear relational abstractions for  operations (such as matrix inverse and sorting-based summation) that appear in the rendering algorithm and careful engineering of the composition of these abstractions in the Crown bound propagation framework~\cite{zhang2018efficient} to ensure tightness of the final abstract images.

Previous work~\cite{AbstractRendering_Neurips2025,Habeeb-AbstractImages24}  demonstrated the feasibility  of abstract rendering in certifying the robustness of vision models such as image classifiers and object detectors against {\em small ranges\/} of semantic purterbations, such as planar camera movements around a target object  and modest lighting changes. For larger-scale applications, such as verifying vision-based controllers operating in large scenes, abstract rendering needs to be integrated into a  verification workflow that can effectively specify, partition, and manage the large number of abstract rendering queries that arise in such settings.

%\chenxi{updated below paragraph}

This paper introduces the open-source  {\en Abstract Rendering Toolkit ($\ART$)} and a set of visual verification benchmark problems to the formal verification community. The toolkit provides a user interface for defining 3D scenes with semantic uncertainties, specifying camera pose spaces, and controlling variations in Gaussian-based scene representations. The main contributions of this work are as follows:

First, to the best of our knowledge, this is the first tool capable of verifying the robustness of a complete vision pipeline (Gaussian splatting + GateNet) in large-scale 3D scenes over 3-dimension pose space, supporting environments with up to 500k splats while providing certifiable accuracy at the 10\,cm level.

Second, we provide a flexible and modular infrastructure that enables researchers to systematically evaluate their methods on our dataset. Specifically, the framework allows users to: (i) explore different rendering algorithms on the same scene representation, (ii) apply and compare different abstraction methods using the same rendering pipeline and scene, and (iii) leverage our pre-tested scene data as a benchmark for reproducible evaluation. Together, these contributions offer both a first-of-its-kind verification tool and a practical benchmark for research in abstract rendering and pose estimation.

Finally, compared with prior work~\cite{AbstractRendering_Neurips2025}, our approach supports a more general definition of planar motion by representing trajectories as sequences of waypoints, each associated with a local pose cell, enabling structured exploration of the pose space.
