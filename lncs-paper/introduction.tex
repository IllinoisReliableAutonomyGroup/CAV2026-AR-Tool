\section{Introduction}
\label{sec:introduction}

Certification of vision-based systems under semantic uncertainties is an important and challenging problem with applications in safety-critical domains such as automotive (vision-based lane keeping~\cite{yin2025certified,9852797} and emergency braking~\cite{yin2025certified}), avionics (vision-based navigation and landing~\cite{semerikov2025vision,santa2022nnlander,li2023refiningperceptioncontractscase}), and robotics (vision-based manipulation~\cite{hu2022robustness}). Semantic uncertainties refer to variations in the 3D scene and camera parameters that affect the rendered images, such as changes in object positions, lighting conditions, and camera viewpoints. Traditional robustness verification techniques for vision models primarily focus on pixel-level perturbations (e.g., adversarial attacks~\cite{szegedy2014intriguingpropertiesneuralnetworks,kurakin2017adversarialexamplesphysicalworld,madry2018towards,8294186,9614158}), which do not capture the rich set of semantic variations encountered in real-world scenarios.
With the emergence of 3D reconstruction techniques such as Neural Radiance Fields (NeRFs)~\cite{10.1145/3503250} and 3D Gaussian Splats (3DGS)~\cite{10.1145/3592433}, it has become possible to learn detailed 3D representations of scenes from images and render novel views. This not only enabled new ways of synthesizing training data for vision models and robotics simulators~\cite{falcon} but also opened up avenues for verifying vision models under semantic uncertainties by reasoning about the underlying 3D scenes and camera parameters.


Abstract rendering~\cite{AbstractRendering_Neurips2025} is a recently developed technique for over-approximating the set of all possible rendered images that can result from a set of camera poses and 3D scenes with semantic uncertainties. In a nutshell, abstract rendering takes as input a bounded set of camera poses---defined by interval pose matrices---and a set of scenes represented by 3D Gaussian spats (3DGS)~\cite{10.1145/3592433}, and propagates these sets through the standard 3DGS rendering algorithm to compute the output set of {\em abstract images}, which are represented by per-pixel color intervals or linear bounds. The key enabler for this technique is a set of piecewise linear relational abstractions for certain primitive nonlinear operations (such as matrix inverse and sorting-based summation) that appear in the rendering algorithm and careful engineering of the composition of these abstractions in the Crown~\cite{zhang2018efficient} bound propagation framework to ensure tightness of the final abstract images.

Previous work~\cite{AbstractRendering_Neurips2025,Habeeb-AbstractImages24} has demonstrated the feasibility  of abstract rendering in certifying the robustness of image processing models like classifiers and object detectors against {\em small ranges\/} of semantic purterbations, such as planar camera movements around a target object  and lighting changes. For larger-scale applications, such as verifying vision-based controllers operating in large scenes, abstract rendering needs to be integrated into a larger verification workflow that can efficiently specify, partition, and manage the large number of abstract rendering queries that arise in such settings.

\chenxi{updated below paragraph}

This paper introduces an open-source toolkit for abstract rendering tailored to the verification community. The toolkit provides a user-friendly interface for defining 3D scenes with semantic uncertainties, specifying camera parameters, and controlling variations in Gaussian-based scene representations. The main contributions of this work are as follows:

1) To the best of our knowledge, this is the first tool capable of verifying the robustness of a complete vision pipeline (Gaussian splatting + GateNet) in large-scale 3D scenes over 3-dimension pose space, supporting environments with up to 500k Gaussians while providing certifiable accuracy at the 10\,cm level.

2) We provide a flexible and modular infrastructure that enables researchers to systematically evaluate their methods on our dataset. Specifically, the framework allows users to: (i) explore different rendering algorithms on the same scene representation, (ii) apply and compare different abstraction methods using the same rendering pipeline and scene, and (iii) leverage our pre-tested scene data as a benchmark for reproducible evaluation. Together, these contributions offer both a first-of-its-kind verification tool and a practical benchmark for research in abstract rendering and pose estimation.

3) Compared with prior work~\cite{AbstractRendering_Neurips2025}, our approach supports a more general definition of planar motion by representing trajectories as sequences of waypoints, each associated with a local pose cell, enabling structured exploration of the pose space.