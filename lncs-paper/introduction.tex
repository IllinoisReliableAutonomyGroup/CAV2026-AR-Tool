\section{Introduction}
\label{sec:introduction}

Certification of vision-based systems under semantic uncertainties is a central challenge with applications in safety-critical domains such as automotive (lane keeping~\cite{yin2025certified,9852797} and emergency braking~\cite{yin2025certified}), avionics (navigation\- and\- landing \cite{semerikov2025vision,santa2022nnlander,li2023refiningperceptioncontractscase}), and robotic manipulation~\cite{hu2022robustness}. Semantic uncertainties refer to variations in the scene being perceived by the vision system as well as the camera parameters such as its pose. Traditional robustness verification techniques for vision models primarily focus on pixel-level perturbations (e.g., adversarial attacks~\cite{szegedy2013intriguing,kurakin2017adversarialexamplesphysicalworld,madry2017towards,8294186,9614158}), and do not capture the rich set of semantic variations that affect the rendered images dramatically.
With the emergence of 3D reconstruction techniques such as Neural Radiance Fields (NeRFs)~\cite{10.1145/3503250} and 3D Gaussian Splats (3DGS)~\cite{10.1145/3592433}, it has become possible to learn photorealistic 3D representations of scenes from images and render novel views. This progress has both scaled up data generation for vision and robotics~\cite{falcon} and opened the door to formal verification of vision models in photorealistic scenes with semantic uncertainty.


Abstract rendering~\cite{AbstractRendering_Neurips2025} is a  technique for over-approximating the set of all possible rendered images that can result from a set of camera poses and 3D scenes with semantic uncertainties. Abstract rendering takes as input a bounded set of camera poses---defined by interval pose matrices---and a set of scenes represented by 3D Gaussian splats (3DGS)~\cite{10.1145/3592433}, and propagates these sets through the standard 3DGS rendering algorithm to compute the output set of {\em abstract images}, which are represented by per-pixel color intervals or linear bounds. The key enabler for this technique is a set of piecewise linear relational abstractions for  operations (such as matrix inverse and sorting-based summation) that appear in the rendering algorithm and careful engineering of the composition of these abstractions in the CROWN bound propagation framework~\cite{zhang2018efficient} to ensure tightness of the final abstract images.

Prior work~\cite{AbstractRendering_Neurips2025,Habeeb-AbstractImages24} showed abstract rendering can certify vision models (classifiers, detectors) under small semantic perturbationsâ€”planar camera motion around a target and modest lighting changes. Scaling to vision-based controllers in large scenes, however, requires a workflow that can specify, partition, and manage thousands of abstract-rendering queries. %Closing that gap, this paper makes two contributions:

%\chenxi{updated below paragraph}

This paper introduces the open-source  {\em Abstract Rendering Toolkit ($\ART$)} and a set of visual verification benchmark problems. The toolkit provides a user interface for defining 3D scenes and camera pose spaces, and it provides the interfaces for partitioning the input spaces and managing the end-to-end verification workflow. The benchmark problems focus on certifying the robustness of vision models in  realistic drone racing scenes with multiple gates spanning over 500k Gaussians each and covering a physical space of over 10 meters in length (see Fig~\ref{fig:scene}). 
The main contributions of this work are as follows:

\begin{itemize}
    \item To the best of our knowledge, this is the first tool to demonstrate formal verification of the robustness of a vision model (GateNet~\cite{9636207}) in large-scale 3DGS scenes with up to 500k Gaussians while providing certifiable accuracy of 10\,cm.
    \item  We release a benchmark suite that pairs four photorealistic 3DGS racing tracks, nominal trajectories, and pose-tube specifications with ready-to-run scripts. Researchers can plug in alternative renderers or abstraction methods and obtain comparable robustness results on identical scenes, enabling reproducible evaluation and head-to-head comparison of new ideas in verification of vision models.
\end{itemize}

% Sayan: Seems minor.
% Finally, compared with prior work~\cite{AbstractRendering_Neurips2025}, our approach supports a more general definition of planar motion by representing trajectories as sequences of waypoints, each associated with a local pose cell, enabling structured exploration of the pose space.
