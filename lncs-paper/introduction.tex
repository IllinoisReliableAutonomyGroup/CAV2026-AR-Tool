\section{Introduction}
\label{sec:introduction}

Certification of vision-based systems under semantic uncertainties is an important and challenging problem with applications in safety-critical domains such as automotive (vision-based lane keeping~\cite{} and emergency braking~\cite{}), avionics (vision-based navigation and landing~\cite{}), and robotics (vision-based manipulation~\cite{}). Semantic uncertainties refer to variations in the 3D scene and camera parameters that affect the rendered images, such as changes in object positions, lighting conditions, and camera viewpoints. Traditional robustness verification techniques for vision models primarily focus on pixel-level perturbations (e.g., adversarial attacks~\cite{}), which do not capture the rich set of semantic variations encountered in real-world scenarios.
With the emergence of 3D reconstruction techniques such as Neural Radiance Fields (NeRFs)~\cite{} and 3D Gaussian Splats (3DGS)~\cite{3DGS}, it has become possible to learn detailed 3D representations of scenes from images and render novel views. This not only enabled new ways of synthesizing training data for vision models and robotics simulators~\cite{} but also opened up avenues for verifying vision models under semantic uncertainties by reasoning about the underlying 3D scenes and camera parameters.


Abstract rendering~\cite{AbstractRendering_Neurips2025} is a recently developed technique for over-approximating the set of all possible rendered images that can result from a set of camera poses and 3D scenes with semantic uncertainties. In a nutshell, abstract rendering takes as input a bounded set of camera poses---defined by interval pose matrices---and a set of scenes represented by 3D Gaussian spats (3DGS)~\cite{3DGS}, and propagates these sets through the standard 3DGS rendering algorithm to compute the output set of {\em abstract images}, which are represented by per-pixel color intervals or linear bounds. The key enabler for this technique is a set of piecewise linear relational abstractions for certain primitive nonlinear operations (such as ...) that appear in the rendering algorithm and careful engineering of the composition of these abstractions in the Crown~\cite{zhang2018efficient} bound propagation framework to ensure tightness of the final abstract images.

Previous work~\cite{AbstractRendering_Neurips2025,Prabhakar} has demonstrated the feasibility  of abstract rendering in certifying the robustness of image processing models like classifiers and object detectors against {\em small ranges\/} of semantic purterbations, such as planar camera movements around a target object  and lighting changes. For larger-scale applications, such as verifying vision-based controllers operating in large scenes, abstract rendering needs to be integrated into a larger verification workflow that can efficiently specify, partition, and manage the large number of abstract rendering queries that arise in such settings.

This paper introduces an open-source toolkit for abstract rendering to the verification community. Our toolkit provides a user-friendly interface for defining 3D scenes with semantic uncertainties, specifying camera parameters, 
\sayan{Highlights, results, technical challenges.}

\chenxi{To the best of our knowledge, this work presents the first tool capable of verifying the robustness of a pose estimation pipeline (gaussian splatting + gatenet) in large-scale 3D scenes, handling environments of up to 500k Gaussians while providing certified accuracy at the 10 cm level. In addition, we provide a flexible and modular infrastructure that enables researchers to systematically evaluate their methods on our dataset. Specifically, our framework allows users to (1) explore different rendering algorithms on the same scene representation, (2) apply and compare different abstraction methods using the same rendering pipeline and scene, and (3) use our pre-tested scene data as a benchmark for reproducible evaluation. Together, these contributions provide both a first-of-its-kind verification tool and a practical benchmark for abstract rendering and pose estimation research.}
