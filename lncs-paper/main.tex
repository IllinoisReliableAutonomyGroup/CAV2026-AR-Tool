\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta,calc}

\input{macros}

\begin{document}

\title{Abstract Rendering Toolkit for}
\author{Author Name}
\institute{Your Institution}

\maketitle

\begin{abstract}
    \sayan{Write later.}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Abstract rendering~\cite{AbstractRendering_Neurips2025} is a recently developed technique for over-approximating the set of all possible rendered images that can result from a set of camera poses and 3D scenes with semantic uncertainties. In a nutshell, abstract rendering takes as input a bounded set of camera poses---defined by interval pose matrices---and a set of scenes represented by 3D Gaussian spats (3DGS)~\cite{3DGS}, and propagates these sets through the standard 3DGS rendering algorithm to compute the output set of {\em abstract images}, which are represented by per-pixel color intervals or linear bounds. The key enabler for this technique is a set of piecewise linear relational abstractions for certain primitive nonlinear operations (such as ...) that appear in the rendering algorithm and careful engineering of the composition of these abstractions in the Crown~\cite{zhang2018efficient} bound propagation framework to ensure tightness of the final abstract images.

Previous work has demonstrated the feasibility  of abstract rendering in certifying the robustness of image processing models like classifiers and object detectors against {\em small ranges\/} of semantic purterbations, such as planar camera movements around a target object  and lighting changes. For larger-scale applications, such as verifying vision-based controllers operating in large scenes, abstract rendering needs to be integrated into a larger verification workflow that can efficiently specify, partition, and manage the large number of abstract rendering queries that arise in such settings.

This paper introduces an open-source toolkit for abstract rendering to the verification community. Our toolkit provides a user-friendly interface for defining 3D scenes with semantic uncertainties, specifying camera parameters, 
\sayan{Highlights, results, technical challenges.}

\section{Using the Abstract Rendering Toolkit}
\label{sec:art}

\paragraph{Running example and workflow.}
We use the task of verifying a vision-based pose estimator, GateNet~\cite{}, over a large pose space in a drone racing arena as a running example in this paper.
\sayan{Figure for running example, gates, track, pose volume around in gray, sample points, example images inset, sample pose volume, abstract image, output pose interval matrix.}


\begin{figure}[ht]
    \centering
    \input{figures/block.tex}
    \caption{Abstract rendering workflow from inputs to GateNet verification output.}
    \label{fig:art-workflow}
\end{figure}

There are several steps and user-interfaces involved in performing such a large-scale verification task: (1) the user has to specify the input pose and scene spaces, (2) these spaces have to be then partitioned into smaller cells according to user-defined parameters, (3)  the abstract rendering engine is then run on these cells and the resulting abstract images are stored in a dictionary, (4) each abstract image from the dictionary is  passed through GateNet using a bound propagation tool like Crown~\cite{} to verify whether the pose estimator is robust over that cell, and (5) finally the verification results are collected for analysis and visualization. Figure~\ref{fig:art-workflow} illustrates this workflow. In the following paragraphs, we describe the modules of the toolkit that support each of these steps.

\paragraph{Input pose and scene spaces.}
\sayan{Fill in details; try to be precise, use math where it makes sens, e.g., pose space is a subset of SE(3), etc., but dont go overboard. Use the running example, refer to the figure of the track and the pose-space to convey the main ideas.}


A planner yields a nominal trajectory $\gamma:[0,T]\to\mathbb{R}^3$ as waypoints $(p_i)$ with a smooth camera orientation field $R:[0,T]\to SO(3)$. Operational uncertainty is a tube
\[
\mathcal{P} = \{(p,R(t)) \mid p \in B(p(t),\Delta_t),\, t\in[0,T]\} \subset SE(3),
\]
where $\Delta_t$ encodes tracking error budgets. This bounded subset of $SE(3)$ is the pose space given to the toolkit.

\paragraph{1) Pose standard and validation.}
Poses are represented as homogeneous transforms $\begin{bmatrix}R&p\\0&1\end{bmatrix}$ with per-waypoint translation tubes $B(p_i,\Delta_i)$ and optional attitude jitter. The validator rejects non-orthogonal $R$, clamps drift, and ensures tube continuity, so downstream bounds remain well-conditioned.

Input pose space is a subset of SE(3)) and user defines it by specifying a nominal trajectory (json file) 
with 6D waypoint, radius information, travel distance along the cynder. A sequence of such entries define the pose space. 
Variations in the scene space in the config file (below).


\paragraph{2) AR Config file (yaml)}

Define 
partitioning parameters for each dimension of SE(3),
paths and file for the 3DGS scene, 
camera intrinsics,
addition parameters related to AR bound propagation (tile size). 

\paragraph{3) Pose/scene partitioning.}
Uses partition paramers from AR config to split the input pose tube $\mathcal{P}$ and scene uncertainties into smaller cells for focused abstract rendering. 
Saves the resulting cells to disk for reuse.

\paragraph{4) Run AR get AI for each cell and storing it in the dictionary. 
Saved in file.}

\paragraph{5) Run Crown gatetent on each cell/abstract image.}

\paragraph{6) pose error check for each cell.}

\paragraph{Auxiliary user function: plotting, etc.}


The pose/scene tube is split into solver-ready regions. Splits are anisotropic: finer near gates or high-curvature segments, coarser on straights. Users pick a budget (\verb|--cells 200|) or tolerance (e.g., $0.15$\,m / $2^\circ$); the tool refines only where image Jacobians w.r.t. pose exceed a threshold, keeping AR tight without exploding cell count.

\paragraph{3) AR results dictionary.}
For each region the AR engine stores alpha bounds, color bounds, and lower/upper images keyed by (pose cell, scene cell, AR config, model hash). This cache avoids rerunning heavy rendering and supports instant plotting (\verb|ar plot --cell 42|) to visualize appearance variability near a gate.

\paragraph{4) Verification dictionary and downstream use.}
GateNet (or any consumer) reads the cached envelopes and records per-cell outcomes: verified / failed / unknown. Keys align with the AR dictionary, so failures trace back to their pose tube and gate. Verdicts can be overlaid on the trajectory via \verb|plot_gatenet.py|, yielding a green (verified) / red (failed) lap-level map.

\paragraph{5) Final error tolerance in pose estimation calculation based on intervals}
Input pose cell - output cell defines pose estimation error tolerance. 

\paragraph{Workflow.}
\begin{enumerate}
    \item \verb|pose validate traj.json --tube 0.2| builds the $SE(3)$ tube.
    \item \verb|pose partition --budget 180| generates focused cells near gates.
    \item \verb|ar run --cells all| populates the AR dictionary.
    \item \verb|verify gatenet --cells all| emits robustness verdicts.
    \item \verb|plot_gatenet.py| visualizes per-cell results along the flight path.
\end{enumerate}

\paragraph{Technical highlights.}
Conditioning of $R$ is preserved during interpolation, avoiding $\det R\neq 1$ drift; anisotropic partitioning targets pose directions with high image sensitivity; cached envelopes are compressed without widening bounds; per-cell verdicts compose to a lap-level statement while accounting for overlapping tubes.



\input{experiments.tex}




\end{document}
