\section{Using the Abstract Rendering Toolkit}
\label{sec:art}

We will discuss the main components of the Abstract Rendering Toolkit (ART) using a running example of certifying the robustness of a GateNet~\cite{9636207} vision-based pose estimator for drone racing under pose and scene uncertainties.

\paragraph{Running example and scope.}
In this application, a drone equipped with a forward-facing camera must navigate through a sequence of gates in a racing arena. GateNet is a neural network that takes as input an RGB image from the drone's camera and outputs a 6-DoF {\em pose estimate} $\hat{y} \in SE(3)$ which includes the 3D position of the drone in the arena and its orientation (roll, pitch, yaw). This pose estimate is used by the drone's controller to adjust its flight path and successfully pass through the gates. Inaccuracies in the pose estimate can lead to collisions with the gates or failure to pass through them. For this example, we use $\ART$ for certifying GateNet's robustness over a user-provided bounded set $P \subset SE(3)$ of possible drone poses around a nominal trajectory that passes through the gates. Additionally, ART can also certify models considering semantic uncertainties in the scene, such as variations in gate colors or lighting conditions.


Figure~\ref{fig:art-workflow} summarizes the stages of the verifcation process using $\ART$ which are as follows: (1) User specifies the pose space $P$ in a JSON file and the scene file along with camera and algorithmic parameters in a YAML configuration file. (2) $\ART$ partitions $P$ into smaller cells $\{P_i\}$ and creates a dictionary of these pose cells. (3) For each pose cell $P_i$, $\ART$ performs abstract rendering to compute the corresponding abstract image $A_i$ and stores it in a dictionary. (4) Each abstract image $A_i$ is fed to GateNet using the CROWN~\cite{zhang2018efficient} verifier to obtain interval bounds on the predicted pose $\hat{Y}_i$, which are stored in a third dictionary.  Finally, (5) for each cell $P_i$, the predicted pose bounds $\hat{Y}_i$ are checked against the input pose cell $P_i$ (with a user-specified tolerance) to determine if GateNet is robust over that cell. In what follows, we describe each of these stages in more detail.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/circle.png}
    \includegraphics[width=0.45\textwidth]{figures/abstract_viz_000000.png}
    \caption{\small Left: nominal trajectory (black) through blue racing gates. The user-defined input pose space $P$ is shown as a cyan tube. Right: {top-left} a sample image rendered in the 3DGS scene from a particular camera pose $p \in P$. Bottom-left and right images show the  abstract image corresponding to a cell $P_i \subseteq P$ that contains $p$ and is computed by our abstract rendering engine. Top-right \sayan{??}.}
\end{figure}

\begin{figure}[ht]
    \centering
    \input{figures/block.tex}
    \caption{Abstract rendering workflow from inputs to GateNet verification output.}
    \label{fig:art-workflow}
\end{figure}

\paragraph{(1) User inputs: Pose space and key analysis parameters.}
The user specifies a pose space $P$ which is a bounded subset of $P \subseteq SE(3)$. For many control applications including drone racing, it is convenient to define $P$ as neighborhood around a set of trajectories that the agent is expected to follow. In our running example, the user provides a nominal trajectory as a sequence of waypoints (6D poses) that pass through the gates, along with tolerances on translation and orientation at each waypoint to define a tube around the trajectory (\sayan{shown in Figure~\ref{fig:art-workflow}}). This defines a  sequence of cylinders that follow the nominal trajectory. Each cylinder is defined by seven numbers: a 6D center \((p,R)\) giving translation \(p\in\mathbb{R}^3\) and nominal orientation \(R\in SO(3)\), a direction/length \(\ell\) along the trajectory, and a radius \(r\) capturing translational uncertainty. A JSON file lists about \sayan{$10^2$?} such cylinders; concatenating them forms the full tube $P\subset SE(3)$. 

\paragraph{Configuration file (YAML).}
An abstract rendering config file supplies: (i) the scene file for the 3DGS; (ii) camera intrinsic parameters (e.g., focal length, image size $W \times H$, optical center); (iii) partitioning parameters for the input pose space $P$ (cell budget or per-dimension split counts over \(SE(3)\)); and (iv) abstract-rendering knobs (tile size for GPU tiling, choice of bound-propagation method in CROWN). These parameters steer fidelity and runtime but are kept high-level in the paper; low-level defaults ship with the code.
Scene variations such as saturation or hue shifts, or alternate gate colors, are optional parameters in the same config file; by default they are fixed constants.



\paragraph{(2) Partitioning module.}
Given the tube \(\mathcal{P}\) and partitioning parameters, the toolkit constructs a queue/dictionary of pose cells. Splits are anisotropic: finer near gates and sharp yaw/roll changes, coarser on straight segments. Each cell stores bounds on translation \([p_{\min},p_{\max}]\) and orientation \([R_{\min},R_{\max}]\) (recorded as intervals on Euler angles or quaternions). Cells are serialized to disk so later stages can resume without re-partitioning. Scene partitioning is off by default but can be enabled if users add lighting or material ranges in the YAML.

\paragraph{Abstract rendering (AR) per cell.}
For every cell, the AR engine propagates pose and scene bounds through 3DGS rendering, producing interval images: per-pixel lower/upper RGBA (or linear bounds). Outputs are cached in an “AR results dictionary”, keyed by (pose cell id, scene id, AR config hash). Each entry stores: (1) the cell bounds; (2) alpha bounds; (3) color bounds; and (4) lower/upper images. Caching prevents rerunning heavy rendering when only the downstream model changes.

\paragraph{Verification stage (GateNet + CROWN).}
Each abstract image is fed to GateNet using CROWN. The verifier returns interval bounds on the predicted pose (three translations plus three orientation parameters). The result is recorded in a “verification dictionary” with status \{verified, failed, unknown\} and the corresponding pose intervals. Keys align with the AR dictionary, enabling backtraces from a failed cell to its rendering evidence.

\paragraph{Pose error check and tolerance.}
For a cell to be verified, the GateNet output intervals must lie inside the input tube segment for that cell (or within a user-specified tolerance). Otherwise the cell is flagged failed; unknown marks cases where bound propagation is inconclusive.

% \paragraph{Workflow for the drone-racing example.}
% \begin{enumerate}
%     \item \verb|pose validate traj.json| checks the \(SE(3)\) cylinders, orthonormalizes \(R\), and reports gaps or overlong segments.
%     \item \verb|pose partition --budget 200| refines cells near gates and high-curvature turns, saves the cell dictionary.
%     \item \verb|ar run --cells all| renders abstract images for each cell and writes the AR dictionary.
%     \item \verb|verify gatenet --cells all| runs CROWN on cached images, writes verification statuses.
%     \item \verb|plot_gatenet.py| overlays per-cell verdicts (green verified, red failed, gray unknown) on the 3D trajectory.
% \end{enumerate}

\paragraph{Technical choices and challenges.}
\emph{SE(3) conditioning.} We normalize \(R\) after interpolation to keep \(\det R=1\), avoiding drift across long tubes.  
\emph{Granularity vs.\ cost.} Anisotropic splitting prevents a blow-up in cell count while keeping abstract images tight where the view is most sensitive.  
\emph{Deterministic storage.} Dictionaries are hashed by pose bounds and config to make runs reproducible and diffable.  
\emph{Extensibility.} Scene variation hooks (lighting, gate colors) live in the YAML so future experiments add semantic perturbations without changing code.  
\emph{Traceability.} Shared keys across the AR and verification dictionaries let users jump from a failed GateNet cell back to its rendered bounds.

